---
---

@string{aps = {American Physical Society,}}

@article{mickel2023importance,
  bibtex_show={show},
  title={The Importance of Multi-Dimensional Intersectionality in Algorithmic Fairness and AI Model Development},
  author={Mickel, Jennifer},
  school={The University of Texas at Austin},
  journal={Polymathic Scholars Honors Thesis},
  abstract={People are increasingly interacting with artificial intelligence (AI) systems and algorithms, but oftentimes, these models are embedded with unfair biases. These biases can lead to harm when an AI system’s output is implicitly or explicitly racist, sexist, or derogatory. If the output is offensive to a person interacting with it, it can cause the person emotional harm that may manifest physically. Alternatively, if a person agrees with the model’s output, the person’s negative biases may be reinforced, inciting the person to engage in discriminatory behavior. Researchers have recognized the harm AI systems can lead to, and they have worked to develop fairness definitions and methodologies for mitigating unfair biases in machine learning models. Unfortunately, these definitions (typically binary) and methodologies are insufficient for preventing AI models from learning unfair biases. To address this, fairness definitions and methodologies must account for intersectional identities in multicultural contexts. The limited scope of fairness definitions allows for models to develop biases against people with intersectional identities that are unaccounted for in the fairness definition. Existing frameworks and methodologies for model development are based in the US cultural context, which may be insufficient for fair model development in different cultural contexts. To assist machine learning practitioners in understanding the intersectional groups affected by their models, a database should be constructed detailing the intersectional identities, cultural contexts, and relevant model domains in which people may be affected. This can lead to fairer model development, for machine learning practitioners will be better adept at testing their model's performance on intersectional groups.},
  year={2023},
  pdf={https://doi.org/10.26153/tsw/49447},
  slides={https://docs.google.com/presentation/d/1I78DQTOZdRO-eQT1eV2TYaObk6MejwJIwUtycsFiGwY/edit?usp=sharing}
}
