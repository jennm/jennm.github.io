---
---

@string{aps = {American Physical Society,}}

@inproceedings{mickel2025more,
  bibtex_show={show},
  title={More of the Same: Persistent Representational Harms Under Increased Representation},
  author={Mickel, Jennifer and De-Arteaga, Maria and Liu, Leqi and Tian, Kevin},
  journal={NeurIPS},
  year={2025},
  eprint={2503.00333},
  archivePrefix={arXiv},
  url={https://neurips.cc/virtual/2025/loc/san-diego/poster/118054},
  abstract={To recognize and mitigate the harms of generative AI systems, it is crucial to consider whether and how different societal groups are represented by these systems. A critical gap emerges when naively measuring or improving who is represented, as this does not consider how people are represented. In this work, we develop GAS(P), an evaluation methodology for surfacing distribution-level group representational biases in generated text, tackling the setting where groups are unprompted (i.e., groups are not specified in the input to generative systems). We apply this novel methodology to investigate gendered representations in occupations across state-of-the-art large language models. We show that, even though the gender distribution when models are prompted to generate biographies leads to a large representation of women, even representational biases persist in how different genders are represented. Our evaluation methodology reveals that there are statistically significant distribution-level differences in the word choice used to describe biographies and personas of different genders across occupations, and we show that many of these differences are associated with representational harms and stereotypes. Our empirical findings caution that naively increasing (unprompted) representation may inadvertently proliferate representational biases, and our proposed evaluation methodology enables systematic and rigorous measurement of the problem.},
  pdf={https://arxiv.org/pdf/2503.00333}
}

@inproceedings{bidermanwrite,
  bibtex_show={show},
  title={Write Code that People Want to Use},
  author={Biderman, Stella and Mickel, Jennifer and Abbasi, Baber},
  booktitle={Championing Open-source DEvelopment in ML Workshop@ ICML25},
  year={2025},
  url={https://openreview.net/forum?id=oH0XhgzJt0},
  abstract={‚ÄúResearch code‚Äù is a common, often self-effacing, term used to refer to the type of code that is commonly released alongside research papers. Research code is notorious for being fragile, poorly documented, and difficult for others to run or extend. In this position paper, we argue that, while research code seems to meet the short-term needs of research projects, in fact the practice hurts researchers by limiting the impact of their work and causing fewer people to build on their research. We explore the structural incentives and dynamics of the field that drive these behaviors. We argue that extensibility matters far more than strict reproducibility for research impact, and propose both pragmatic approaches for individual researchers and institutional reforms to encourage the development of more usable and maintainable research software.},
  pdf={https://openreview.net/pdf?id=oH0XhgzJt0},
}

@article{chang2025global,
  bibtex_show={show},
  title={Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures},
  author={Tyler A. Chang and Catherine Arnett and Abdelrahman Eldesokey and Abdelrahman Boda Sadallah and Abeer Kashar and Abolade Daud and Abosede Grace Olanihun and Adamu Labaran Mohammed and Adeyemi Praise and Adhikarinayum Meerajita Sharma and Aditi Gupta and Afitab Iyigun and Afonso Simpl'icio and Ahmed Essouaied and Aicha Chorana and Akhil Eppa and Akintunde Oladipo and Akshay Ramesh and Aleksei Dorkin and Alfred Malengo Kondoro and Alham Fikri Aji and Ali Eren cCetintacs and Allan Hanbury and Alou Demb{\'e}l{\'e} and Alp Niksarli and 'Alvaro Arroyo and Amin Bajand and Amol Khanna and Ana Chkhaidze and Ana Carolina Condez and Andiswa Mkhonto and Andrew Hoblitzell and Andrew Tran and Angelos Poulis and Anirban Majumder and Anna Vacalopoulou and Annette Kuuipolani Kanahele Wong and Annika Simonsen and Anton Kovalev and Ashvanth.S and Ayodeji Joseph Lana and Barkin Kinay and Bashar Alhafni and Benedict Cibalinda Busole and Bernard Ghanem and Bharti Nathani and Biljana Stojanovska DJuri'c and Bola Agbonile and Bragi Bergsson and Bruce Torres Fischer and Burak Tutar and Burcu Alakucs cCinar and Cade J. Kanoniakapueo Kane and Can Udomcharoenchaikit and Chadi Helwe and Chaithra Reddy Nerella and Chen Cecilia Liu and Chiamaka Glory Nwokolo and Cristina Espa{\~n}a-Bonet and Cynthia Amol and DaeYeop Lee and Dana Arad and Daniil Dzenhaliou and Daria Pugacheva and Dasol Choi and Daud Olamide Abolade and David Liu and David Semedo and Deborah Popoola and Deividas Mataciunas and Delphine Nyaboke and Dhyuthy Krishna Kumar and Diogo Gl'oria-Silva and Diogo Tavares and Divyanshu Goyal and DongGeon Lee and Ebele Nwamaka Anajemba and Egonu Ngozi Grace and Elena Mickel and Elena Tutubalina and Elias Herranen and Emile Anand and Emmanuel Habumuremyi and Emuobonuvie Maria Ajiboye and Eryawan Presma Yulianrifat and Esther Adenuga and Ewa Rudnicka and Faith Olabisi Itiola and Faran Taimoor Butt and Fathima Thekkekara and Fatima Haouari and Filbert Aurelian Tjiaranata and Firas Laakom and Francesca Grasso and Francesco Orabona and Francesco Periti and Gbenga Kayode Solomon and Gia Nghia Ngo and Gloria Udhehdhe-oze and Gonccalo Vinagre Martins and Gopi Naga Sai Ram Challagolla and Guijin Son and Gulnaz Abdykadyrova and Hafsteinn Einarsson and Hai Hu and Hamidreza Saffari and Hamza Zaidi and Haopeng Zhang and Harethah Abu Shairah and Harry Vuong and Hele-Andra Kuulmets and Houda Bouamor and Hwanjo Yu and Iben Nyholm Debess and .Ibrahim Ethem Deveci and Ikhlasul Akmal Hanif and Ikhyun Cho and Ines Calvo and Ines Vieira and Isaac Manzi and Ismail Daud and Itay Itzhak and Iuliia Alekseenko and Ivan Belashkin and Ivan Spada and Ivan Zhelyazkov and Jacob Brinton and Jafar Isbarov and Jaka vCibej and Jan vCuhel and Jan Koco'n and Jauza Akbar Krito and Jebish Purbey and Jennifer Mickel and Jennifer Za and Jenny Kunz and Jihae Jeong and Jimena Tena D'avalos and Jinu Lee and Joao Magalhaes and John Yi and Jongin Kim and Joseph Chataignon and Joseph Marvin Imperial and Jubeerathan Thevakumar and Judith Land and Junchen Jiang and Jungwhan Kim and Kairit Sirts and R Kamesh and V Kamesh and Kanda Patrick Tshinu and K{\"a}triin Kukk and Kaustubh Ponkshe and Kavsar Huseynova and Ke He and Kelly Buchanan and Kengatharaiyer Sarveswaran and Kerem Zaman and Khalil Mrini and Kian Kyars and Krister Kruusmaa and Kusum Chouhan and Lainitha Krishnakumar and Laura Castro S'anchez and Laura Porrino Moscoso and Leshem Choshen and Levent Sencan and Lilja Ovrelid and Lisa Alazraki and Lovina Ehimen-Ugbede and Luheerathan Thevakumar and Luxshan Thavarasa and Mahnoor Malik and Mamadou K. Keita and Mansi Jangid and Marco De Santis and Marcos Garc'ia and Marek Suppa and Mariam D'Ciofalo and Marii Ojastu and Maryam Sikander and Mausami Narayan and Maximos Skandalis and Mehak Mehak and Mehmet .Ilterics Bozkurt and Melaku Bayu Workie and Menan Velayuthan and Michael Leventhal and Michal Marci'nczuk and Mirna Potovcnjak and Mohammadamin Shafiei and Mridul Sharma and Mrityunjaya Indoria and Muhammad Ravi Shulthan Habibi and Murat Koli'c and Nada Galant and Naphat Permpredanun and Narada Maugin and Nicholas Kluge Correa and Nikola Ljubevsi'c and Nirmal Thomas and Nisansa de Silva and Nisheeth Joshi and Nitish Ponkshe and Nizar Habash and Nneoma Chinemerem Udeze and Noel Thomas and No{\'e}mi Ligeti-Nagy and Nouhoum Souleymane Coulibaly and Nsengiyumva Faustin and Odunayo Kareemat Buliaminu and Odunayo Ogundepo and Oghojafor Godswill Fejiro and Ogundipe Blessing Funmilola and Okechukwu God'spraise and Olanrewaju Samuel and Olaoye Deborah Oluwaseun and Olasoji Akindejoye and Olga Popova and Olga Snissarenko and Onyinye Anulika Chiemezie and Orkun Kƒ±nay and Osman Tursun and Owoeye Tobiloba Moses and Oyelade Oluwafemi Joshua and Oyesanmi Fiyinfoluwa and Pablo Gamallo and Pablo Rodr'iguez Fern'andez and Palak Arora and Pedro Valente and Peter Rupnik and Philip Oghenesuowho Ekiugbo and Pramit Sahoo and Prokopis Prokopidis and Pua Niau-Puhipau and Quadri Yahya and Rachele Mignone and Raghav Singhal and Ramyakrishna Kadiyala and Raphael Merx and Rapheal Afolayan and Ratnavel Rajalakshmi and Rishav Ghosh and Romina Oji and Ron Kekeha Solis and Rui Guerra and Rushikesh Zawar and Sa'ad Nasir Bashir and Saeed Alzaabi and Sahil Sandeep and Sailaja Batchu and Sai Nithin Reddy Kantareddy and Salsabila Zahirah Pranida and Sam Buchanan and Samuel Rutunda and Sander Land and Sarah Sulollari and Sardar Ali and Saroj Sapkota and Saulius Tautvai{\vs}as and Sayambhu Sen and Sayantani Banerjee and S{\'e}bastien Diarra and SenthilNathan.M and Sewoong Lee and Shaan Shah and Shankar Venkitachalam and Sharifa Djurabaeva and Sharon Ibejih and Shivanya Shomir Dutta and Siddhant Gupta and Silvia Paniagua Su'arez and Sina Ahmadi and Sivasuthan Sukumar and Siyuan Song and A Snegha and Sokratis Sofianopoulos and Sona Elza Simon and Sonja Benvcina and Sophie Gvasalia and Sphurti Kirit More and Spyros Dragazis and Stephan P. Kaufhold and Suba.S and Sultan Alrashed and Surangika Ranathunga and Taiga Someya and Taja Kuzman Pungervsek and Tal Haklay and Tasi'u Jibril and Tatsuya Aoyama and T B Abashidze and Terenz Jomar Dela Cruz and Terra Blevins and Themistoklis Nikas and Theresa Dora Idoko and Thu Mai Do and Tilek Chubakov and Tommaso Gargiani and Uma Rathore and Uni Johannesen and Uwuma Doris Ugwu and Vallerie Alexandra Putra and Vanya Bannihatti Kumar and Varsha Jeyarajalingam and Varvara Arzt and Vasudevan Nedumpozhimana and Viktoria Ondrejova and Viktoryia Horbik and Vishnu Vardhan Reddy Kummitha and Vuk Dini'c and Walelign Tewabe Sewunetie and Winston Wu and Xiaojing Zhao and Yacouba Diarra and Yaniv Nikankin and Yash Mathur and Yixi Chen and Yiyuan Li and Yolanda Xavier and Yonatan Belinkov and Yusuf Ismail Abayomi and Zaid Alyafeai and Zhengyang Shan and Zhi Rui Tam and Zilu Tang and Zuzana Naƒèov{\'a} and Baber Abbasi and Stella Biderman and David Stap and Duygu Ataman and Fabian Schmidt and Hila Gonen and Jiayi Wang and David Ifeoluwa Adelani},
  journal={arXiv preprint arXiv:2510.24081},
  year={2025},
  url={https://arxiv.org/pdf/2510.24081},
  abstract={To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lowerresource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.},
  pdf={https://arxiv.org/pdf/2510.24081}
}

@misc{solaiman2024evaluatingsocialimpactgenerative,
    bibtex_show={show},
    title={Evaluating the Social Impact of Generative AI Systems in Systems and Society}, 
    author={Irene Solaiman and Zeerak Talat and William Agnew and Lama Ahmad and Dylan Baker and Su Lin Blodgett and Canyu Chen and Hal Daum√© III and Jesse Dodge and Isabella Duan and Ellie Evans and Felix Friedrich and Avijit Ghosh and Usman Gohar and Sara Hooker and Yacine Jernite and Ria Kalluri and Alberto Lusoli and Alina Leidinger and Michelle Lin and Xiuzhu Lin and Sasha Luccioni and Jennifer Mickel and Margaret Mitchell and Jessica Newman and Anaelia Ovalle and Marie-Therese Png and Shubham Singh and Andrew Strait and Lukas Struppek and Arjun Subramonian},
    year={2024},
    eprint={2306.05949},
    archivePrefix={arXiv},
    primaryClass={cs.CY},
    url={https://zeerak.org/papers/Evaluating_the_Social_Impact_of_Generative_AI_Systems_in_Systems_and_Society__preprint_.pdf}, 
    abstract={Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.},
    pdf={https://arxiv.org/pdf/2306.05949}
}

@inproceedings{mickel2024racial,
  bibtex_show={show},
  author = {Mickel, Jennifer},
  title = {Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent},
  year = {2024},
  isbn = {9798400704505},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3630106.3659050},
  doi = {10.1145/3630106.3659050},
  abstract = {Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of who comprises the racial categories chosen and how people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied. In this paper, we make two contributions. First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups. Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories.},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {2484‚Äì2494},
  numpages = {11},
  keywords = {algorithmic fairness, race and ethnicity, racial categories, racialization},
  location = {Rio de Janeiro, Brazil},
  series = {FAccT '24},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3630106.3659050},
  slides={https://docs.google.com/presentation/d/1yCsTPBTv9FId0aG1Iq4TGGexIaRp73CPrp0AGTQwpmY/edit#slide=id.gd1bf8d60a4_0_0}
}

@article{mickel2024intersectional,
  bibtex_show={show},
  title={Intersectional Insights for Robust Models: Introducing FOG üò∂‚Äçüå´Ô∏è for Improving Worst Case Performance Without Group Information},
  author={Mickel, Jennifer},
  school={The University of Texas at Austin},
  journal={Turing Scholars Honors Thesis},
  abstract={Standard training through empirical risk minimization (ERM) can result
    in seemingly well-performing models that reach high accuracy on average
    but achieve low accuracy on specific groups. Group-specific low accuracy
    is especially of concern in cases in which groups are underrepresented in
    the training data or when spurious correlations are present within data.
    Furthermore, instances can be a part of multiple groups, as in the case of
    demographic groups. Previous approaches, such as group distributional
    robust optimization (Group DRO), achieve high worst-group accuracy yet
    require group information. Group information is not always available due
    to legal, data quality, or cost constraints. Other approaches not requiring
    group information exist, but gaps between these approaches and group
    DRO persist and seldom consider overlapping groups. We develop a model
    development cycle and algorithm Fog to improve the performance of the
    worst-performing group without group information that accounts for 
    overlapping groups. We first train a model using ERM and utilize the model
    features corresponding with the data to identify groups. We use these
    identified groups with group DRO to train a new model. This process can
    be repeated to improve performance. Using our method, we find that we
    can improve the performance of the worst-performing group compared to
    ERM and other algorithms not requiring group information, such as JTT.},
  year={2024},
  pdf={https://drive.google.com/file/d/1dQAHiaMckSDwnlHYq1FRyZtky7WJqaWm/view?usp=sharing},
  slides={https://docs.google.com/presentation/d/1jKlz_P5VhMGKxwlFJMuOcV2N2AzbSMGVP__l6Ylzs40/edit#slide=id.g708a6ee8a1_0_46}
}

@article{mickel2023importance,
  bibtex_show={show},
  title={The Importance of Multi-Dimensional Intersectionality in Algorithmic Fairness and AI Model Development},
  author={Mickel, Jennifer},
  school={The University of Texas at Austin},
  journal={Polymathic Scholars Honors Thesis},
  abstract={People are increasingly interacting with artificial intelligence (AI) systems and algorithms, but oftentimes, these models are embedded with unfair biases. These biases can lead to harm when an AI system‚Äôs output is implicitly or explicitly racist, sexist, or derogatory. If the output is offensive to a person interacting with it, it can cause the person emotional harm that may manifest physically. Alternatively, if a person agrees with the model‚Äôs output, the person‚Äôs negative biases may be reinforced, inciting the person to engage in discriminatory behavior. Researchers have recognized the harm AI systems can lead to, and they have worked to develop fairness definitions and methodologies for mitigating unfair biases in machine learning models. Unfortunately, these definitions (typically binary) and methodologies are insufficient for preventing AI models from learning unfair biases. To address this, fairness definitions and methodologies must account for intersectional identities in multicultural contexts. The limited scope of fairness definitions allows for models to develop biases against people with intersectional identities that are unaccounted for in the fairness definition. Existing frameworks and methodologies for model development are based in the US cultural context, which may be insufficient for fair model development in different cultural contexts. To assist machine learning practitioners in understanding the intersectional groups affected by their models, a database should be constructed detailing the intersectional identities, cultural contexts, and relevant model domains in which people may be affected. This can lead to fairer model development, for machine learning practitioners will be better adept at testing their model's performance on intersectional groups.},
  year={2023},
  pdf={https://doi.org/10.26153/tsw/49447},
  slides={https://docs.google.com/presentation/d/1I78DQTOZdRO-eQT1eV2TYaObk6MejwJIwUtycsFiGwY/edit?usp=sharing}
}
